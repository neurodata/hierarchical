{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from proglearn import UncertaintyForest as UF\n",
    "from hierarchical import HierarchicalForest\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import torchvision\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class HierarchicalForest:\n",
    "    def __init__(self, n_estimators_coarse=25, n_estimators_fine=10, max_depth=10):\n",
    "        self.n_estimators_coarse=n_estimators_coarse\n",
    "        self.coarse_forest=None\n",
    "        \n",
    "        self.n_estimators_fine=n_estimators_fine\n",
    "        self.fine_forests={}\n",
    "        \n",
    "        self.max_depth=max_depth\n",
    "        \n",
    "        self.fitted=False\n",
    "        \n",
    "    def fit(self, X, y, fine_to_coarse):\n",
    "        self.fine_to_coarse = fine_to_coarse\n",
    "        self.classes = np.unique(y)\n",
    "        self.coarse_labels = np.unique(fine_to_coarse)\n",
    "        \n",
    "        \n",
    "        y_coarse=np.zeros(len(y))\n",
    "        for coarse_label in self.coarse_labels:\n",
    "            temp_fine_indices = np.where(fine_to_coarse == coarse_label)[0]\n",
    "            temp_indices = np.concatenate([np.where(y == self.classes[tfi])[0] for tfi in temp_fine_indices])\n",
    "            \n",
    "            y_coarse[temp_indices] = coarse_label\n",
    "            \n",
    "        self._fit_coarse(X, y_coarse)\n",
    "        self._fit_fine(X, y, y_coarse)\n",
    "        \n",
    "        self.fitted=True\n",
    "        \n",
    "        \n",
    "    def _fit_coarse(self, X, y_coarse):\n",
    "        self.coarse_forest = UF(n_estimators=self.n_estimators_coarse, max_depth=self.max_depth).fit(X, y_coarse)\n",
    "    \n",
    "    \n",
    "    def _fit_fine(self, X, y, y_coarse):\n",
    "        for coarse_label in self.coarse_labels:\n",
    "            temp_indices = np.where(y_coarse == coarse_label)[0]\n",
    "            print(np.unique(y[temp_indices]))\n",
    "            self.fine_forests[coarse_label] = UF(n_estimators=self.n_estimators_fine, max_depth=self.max_depth\n",
    "                                                ).fit(X[temp_indices], y[temp_indices])\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        posteriors = np.zeros((X.shape[0], len(self.classes)))\n",
    "        coarse_posteriors = self.coarse_forest.predict_proba(X, 0)\n",
    "        \n",
    "\n",
    "        #- Hierarchical posteriors & prediction\n",
    "        for i, coarse_label in enumerate(self.coarse_labels):\n",
    "            temp_fine_label_indices = np.where(self.fine_to_coarse == coarse_label)[0]\n",
    "\n",
    "            temp_fine_posteriors = self.fine_forests[coarse_label].predict_proba(X, 0)\n",
    "            posteriors[:, temp_fine_label_indices] = np.multiply(coarse_posteriors[:, i],\n",
    "                                                                         temp_fine_posteriors.T\n",
    "                                                                        ).T\n",
    "        \n",
    "        return posteriors\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "        \n",
    "\n",
    "def get_accuracy(seed, fine_to_coarse, n_trees_coarse, n_trees_fine, max_depth):\n",
    "    \n",
    "    X_train, _, y_train, _ = train_test_split(trainset.data, trainset.targets, test_size=0.9, random_state=seed)\n",
    "    \n",
    "    if fine_to_coarse is None:\n",
    "        f = UF(n_estimators=n_trees_fine, max_depth=max_depth).fit(X_train, y_train)\n",
    "        return (f.predict(testset.data) == testset.targets).mean()\n",
    "    \n",
    "    f = HierarchicalForest(n_estimators_coarse=n_trees_coarse, n_estimators_fine=n_trees_fine, max_depth=max_depth)\n",
    "    f.fit(X_train, y_train, fine_to_coarse)\n",
    "    \n",
    "    return (f.predict(testset.data) == testset.targets).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Data processing 1\n",
    "class Dataset:\n",
    "    def __init__(self, file='cifar_100_Bit_m-r101x1_embd.p', train=True, classes=[]):\n",
    "        if train:\n",
    "            if file == '../../../data/cifar_100_Bit_m-r101x1_embd.p':\n",
    "                self.data = pickle.load(open(file, 'rb'))[0][0]\n",
    "                self.targets = np.concatenate(pickle.load(open(file, 'rb'))[0][1])\n",
    "        else:\n",
    "            if file == '../../../data/cifar_100_Bit_m-r101x1_embd.p':\n",
    "                self.data = pickle.load(open(file, 'rb'))[1][0]\n",
    "                self.targets = np.concatenate(pickle.load(open(file, 'rb'))[1][1])\n",
    "        \n",
    "        self.classes = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#- Data processing 2\n",
    "\n",
    "cif100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
    "\n",
    "file='../../../data/cifar_100_Bit_m-r101x1_embd.p'\n",
    "trainset = Dataset(file, train=True, classes=cif100.classes)\n",
    "testset = Dataset(file, train=False, classes=cif100.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Data processing 3\n",
    "\n",
    "data_dimension=2048\n",
    "\n",
    "if data_dimension < trainset.data.shape[1]:\n",
    "    pca = PCA(n_components=data_dimension)\n",
    "    pca.fit(trainset.data)\n",
    "    trainset.data = pca.transform(trainset.data)\n",
    "    testset.data = pca.transform(testset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Data processing 4\n",
    "\n",
    "coarse_to_fine_map = {\n",
    "'aquatic_mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "'food_containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "'fruit_and_vegetables': ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
    "'household_electrical_devices': ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
    "'household_furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "'large_carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "'large_man-made_outdoor_things': ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
    "'large_natural_outdoor_scenes': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
    "'large_omnivores_and_herbivores': ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
    "'medium-sized_mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "'non-insect_invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
    "'vehicles_1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "'vehicles_2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
    "}\n",
    "\n",
    "coarse_number_to_coarse_name = {i: name for i, name in enumerate(coarse_to_fine_map)}\n",
    "\n",
    "def fine_to_coarse(coarse_to_fine):\n",
    "    fine_to_coarse_map = {}\n",
    "    for key in coarse_to_fine:\n",
    "        fines = coarse_to_fine[key]\n",
    "        for f in fines:\n",
    "            fine_to_coarse_map[f] = key\n",
    "            \n",
    "    return fine_to_coarse_map\n",
    "\n",
    "fine_to_coarse_map = fine_to_coarse(coarse_to_fine_map)\n",
    "\n",
    "fine_number_to_fine_name = {i: name for i, name in enumerate(trainset.classes)}\n",
    "fine_name_to_fine_number = {name: i for i, name in fine_number_to_fine_name.items()}\n",
    "\n",
    "for i in range(100):\n",
    "    fine_to_coarse_map[fine_number_to_fine_name[i]]\n",
    "    \n",
    "coarse_name_to_coarse_number = {name: i for i, name in enumerate(coarse_to_fine_map)}\n",
    "\n",
    "coarse_targets = np.array([coarse_name_to_coarse_number[fine_to_coarse_map[fine_number_to_fine_name[y]]] for y in trainset.targets])\n",
    "idx_by_coarse = np.array([np.where(coarse_targets == y)[0] for y in range(20)])\n",
    "idx_by_fine = np.array([np.where(trainset.targets == y)[0] for y in range(100)])\n",
    "\n",
    "\n",
    "test_coarse_targets = np.array([coarse_name_to_coarse_number[fine_to_coarse_map[fine_number_to_fine_name[y]]] for y in testset.targets])\n",
    "test_idx_by_coarse = np.array([np.where(test_coarse_targets == y)[0] for y in range(20)])\n",
    "\n",
    "\n",
    "coarse_names = np.array(list(coarse_name_to_coarse_number.keys()))\n",
    "\n",
    "fine_number_to_coarse_number = {fn: coarse_name_to_coarse_number[\n",
    "                                        fine_to_coarse_map[\n",
    "                                            fine_number_to_fine_name[fn]\n",
    "                                        ]\n",
    "                                    ] for fn in range(100)}\n",
    "\n",
    "\n",
    "fine_by_coarse = [np.where(np.array(list(fine_number_to_coarse_number.values())) == i)[0] for i in range(20)]\n",
    "all_fine = np.concatenate(fine_by_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-458eb3fd3c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mcondensed_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondensed_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtupl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtupl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccuracy_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venvs/hc/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/hc/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/hc/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_mc=10\n",
    "master_seed = 42\n",
    "np.random.seed(master_seed)\n",
    "seeds = np.random.randint(10000, size=n_mc)\n",
    "n_cores=30\n",
    "\n",
    "truth=np.array(list(fine_number_to_coarse_number.values()))\n",
    "\n",
    "tasksim_clusters = pickle.load(open('cifar_tasksim_clusters.pkl', 'rb'))\n",
    "condmean_clusters = pickle.load(open('cifar_condmean_clusters.pkl', 'rb'))\n",
    "\n",
    "n_trees_coarse = 100\n",
    "n_trees_fine = 25\n",
    "\n",
    "max_depths=[10,20,30]\n",
    "\n",
    "accuracy_tuples = []\n",
    "for i, pair_of_clusters in enumerate(zip(tasksim_clusters, condmean_clusters)):\n",
    "    n_classes = len(pair_of_clusters[0])\n",
    "    \n",
    "    n_tasksim_coarse = len(np.unique(pair_of_clusters[0]))\n",
    "    n_condmean_coarse = len(np.unique(pair_of_clusters[0]))\n",
    "    \n",
    "    max_coarse = max([n_tasksim_coarse, n_condmean_coarse])\n",
    "\n",
    "    for j, md in enumerate(max_depths):\n",
    "        #- Hierarchical forest trained using tasksim clusters\n",
    "        accuracy_tuples.append((seeds[i], pair_of_clusters[0], n_trees_coarse, n_trees_fine, md))\n",
    "\n",
    "        #- Hierarchical forest trained using random clusters with the same structure as the tasksim clusters\n",
    "        accuracy_tuples.append((seeds[i], \n",
    "                                pair_of_clusters[0][np.random.choice(n_classes, size=n_classes, replace=False)],\n",
    "                              n_trees_coarse, n_trees_fine, md))\n",
    "\n",
    "\n",
    "        #- Hierarchical forest trained using cond mean clusters\n",
    "        accuracy_tuples.append((seeds[i], pair_of_clusters[1], n_trees_coarse, n_trees_fine, max_depth))\n",
    "\n",
    "        #- Hierarchical forest trained using random clusters with the same structure as the cond mean clusters\n",
    "        accuracy_tuples.append((seeds[i], \n",
    "                                pair_of_clusters[1][np.random.choice(n_classes, size=n_classes, replace=False)],\n",
    "                               n_trees_coarse, n_trees_fine, md))\n",
    "\n",
    "        #- Flat forest\n",
    "        accuracy_tuples.append((seeds[i], None, 0, n_trees_coarse + max_coarse * n_trees_fine, 2*md))\n",
    "    \n",
    "condensed_func = lambda x: get_accuracy(*x)\n",
    "    \n",
    "accuracies = Parallel(n_jobs=n_cores)(delayed(condensed_func)(tupl) for tupl in accuracy_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 0\n",
      "2 0\n",
      "0 1\n",
      "1 1\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "0 3\n",
      "1 3\n",
      "2 3\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "0 5\n",
      "1 5\n",
      "2 5\n",
      "0 6\n",
      "1 6\n",
      "2 6\n",
      "0 7\n",
      "1 7\n",
      "2 7\n",
      "0 8\n",
      "1 8\n",
      "2 8\n",
      "0 9\n",
      "1 9\n",
      "2 9\n",
      "(mean, std err) tasksim accuracy 72.5 13.685454565584099\n",
      "(mean, std err) random tasksim accuracy 73.5 13.685454565584099\n",
      "(mean, std err) condmean accuracy 74.5 13.685454565584099\n",
      "(mean, std err) random condmean accuracy 75.5 13.685454565584099\n",
      "(mean, std err) flat accuracy 76.5 13.685454565584099\n"
     ]
    }
   ],
   "source": [
    "tasksim_accuracies = np.zeros((len(max_depths), n_mc))\n",
    "random_tasksim_accuracies = np.zeros((len(max_depths), n_mc))\n",
    "\n",
    "condmean_accuracies = np.zeros((len(max_depths), n_mc))\n",
    "random_condmean_accuracies = np.zeros((len(max_depths), n_mc))\n",
    "\n",
    "flat_accuracies = np.zeros((len(max_depths), n_mc))\n",
    "\n",
    "for i in range(int(len(accuracies) / 5)):\n",
    "    index = i*5\n",
    "    max_depth_idx = i % 3\n",
    "    mc_idx = int(np.math.floor(i / 3))\n",
    "\n",
    "    tasksim_accuracies[max_depth_idx, mc_idx] = accuracies[index+0]\n",
    "    random_tasksim_accuracies[max_depth_idx, mc_idx] = accuracies[index+1]\n",
    "    condmean_accuracies[max_depth_idx, mc_idx] = accuracies[index+2]\n",
    "    random_condmean_accuracies[max_depth_idx, mc_idx] = accuracies[index+3]\n",
    "    flat_accuracies[max_depth_idx, mc_idx] = accuracies[index+4]\n",
    "\n",
    "print(\"(mean, std err) tasksim accuracy\", np.mean(tasksim_accuracies), np.std(tasksim_accuracies) / np.sqrt(n_mc))\n",
    "print(\"(mean, std err) random tasksim accuracy\", np.mean(random_tasksim_accuracies), np.std(random_tasksim_accuracies) / np.sqrt(n_mc))\n",
    "print(\"(mean, std err) condmean accuracy\", np.mean(condmean_accuracies), np.std(condmean_accuracies) / np.sqrt(n_mc))\n",
    "print(\"(mean, std err) random condmean accuracy\", np.mean(random_condmean_accuracies), np.std(random_condmean_accuracies) / np.sqrt(n_mc))\n",
    "print(\"(mean, std err) flat accuracy\", np.mean(flat_accuracies), np.std(flat_accuracies) / np.sqrt(n_mc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc",
   "language": "python",
   "name": "hc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
