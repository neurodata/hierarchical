{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from proglearn import UncertaintyForest as UF\n",
    "from hierarchical import HierarchicalForest\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import torchvision\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_accuracy(seed, fine_to_coarse, n_trees_coarse, n_trees_fine, max_depth):\n",
    "    \n",
    "    X_train, _, y_train, _ = train_test_split(trainset.data, trainset.targets, test_size=0.9, random_state=seed)\n",
    "    \n",
    "    if fine_to_coarse is None:\n",
    "        f = UF(n_estimators=n_trees_fine, max_depth=max_depth).fit(X_train, y_train)\n",
    "        return (f.predict(testset.data) == testset.targets).mean()\n",
    "    \n",
    "    f = HierarchicalForest(n_estimators_coarse=n_trees_coarse, n_estimators_fine=n_trees_fine, max_depth=max_depth)\n",
    "    f.fit(X_train, y_train, fine_to_coarse)\n",
    "    \n",
    "    return (f.predict(testset.data) == testset.targets).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Data processing 1\n",
    "class Dataset:\n",
    "    def __init__(self, file='cifar_100_Bit_m-r101x1_embd.p', train=True, classes=[]):\n",
    "        if train:\n",
    "            if file == '../../../data/cifar_100_Bit_m-r101x1_embd.p':\n",
    "                self.data = pickle.load(open(file, 'rb'))[0][0]\n",
    "                self.targets = np.concatenate(pickle.load(open(file, 'rb'))[0][1])\n",
    "        else:\n",
    "            if file == '../../../data/cifar_100_Bit_m-r101x1_embd.p':\n",
    "                self.data = pickle.load(open(file, 'rb'))[1][0]\n",
    "                self.targets = np.concatenate(pickle.load(open(file, 'rb'))[1][1])\n",
    "        \n",
    "        self.classes = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#- Data processing 2\n",
    "\n",
    "cif100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
    "\n",
    "file='../../../data/cifar_100_Bit_m-r101x1_embd.p'\n",
    "trainset = Dataset(file, train=True, classes=cif100.classes)\n",
    "testset = Dataset(file, train=False, classes=cif100.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Data processing 3\n",
    "\n",
    "data_dimension=2048\n",
    "\n",
    "if data_dimension < trainset.data.shape[1]:\n",
    "    pca = PCA(n_components=data_dimension)\n",
    "    pca.fit(trainset.data)\n",
    "    trainset.data = pca.transform(trainset.data)\n",
    "    testset.data = pca.transform(testset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Data processing 4\n",
    "\n",
    "coarse_to_fine_map = {\n",
    "'aquatic_mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "'food_containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "'fruit_and_vegetables': ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
    "'household_electrical_devices': ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
    "'household_furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "'large_carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "'large_man-made_outdoor_things': ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
    "'large_natural_outdoor_scenes': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
    "'large_omnivores_and_herbivores': ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
    "'medium-sized_mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "'non-insect_invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
    "'vehicles_1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "'vehicles_2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
    "}\n",
    "\n",
    "coarse_number_to_coarse_name = {i: name for i, name in enumerate(coarse_to_fine_map)}\n",
    "\n",
    "def fine_to_coarse(coarse_to_fine):\n",
    "    fine_to_coarse_map = {}\n",
    "    for key in coarse_to_fine:\n",
    "        fines = coarse_to_fine[key]\n",
    "        for f in fines:\n",
    "            fine_to_coarse_map[f] = key\n",
    "            \n",
    "    return fine_to_coarse_map\n",
    "\n",
    "fine_to_coarse_map = fine_to_coarse(coarse_to_fine_map)\n",
    "\n",
    "fine_number_to_fine_name = {i: name for i, name in enumerate(trainset.classes)}\n",
    "fine_name_to_fine_number = {name: i for i, name in fine_number_to_fine_name.items()}\n",
    "\n",
    "for i in range(100):\n",
    "    fine_to_coarse_map[fine_number_to_fine_name[i]]\n",
    "    \n",
    "coarse_name_to_coarse_number = {name: i for i, name in enumerate(coarse_to_fine_map)}\n",
    "\n",
    "coarse_targets = np.array([coarse_name_to_coarse_number[fine_to_coarse_map[fine_number_to_fine_name[y]]] for y in trainset.targets])\n",
    "idx_by_coarse = np.array([np.where(coarse_targets == y)[0] for y in range(20)])\n",
    "idx_by_fine = np.array([np.where(trainset.targets == y)[0] for y in range(100)])\n",
    "\n",
    "\n",
    "test_coarse_targets = np.array([coarse_name_to_coarse_number[fine_to_coarse_map[fine_number_to_fine_name[y]]] for y in testset.targets])\n",
    "test_idx_by_coarse = np.array([np.where(test_coarse_targets == y)[0] for y in range(20)])\n",
    "\n",
    "\n",
    "coarse_names = np.array(list(coarse_name_to_coarse_number.keys()))\n",
    "\n",
    "fine_number_to_coarse_number = {fn: coarse_name_to_coarse_number[\n",
    "                                        fine_to_coarse_map[\n",
    "                                            fine_number_to_fine_name[fn]\n",
    "                                        ]\n",
    "                                    ] for fn in range(100)}\n",
    "\n",
    "\n",
    "fine_by_coarse = [np.where(np.array(list(fine_number_to_coarse_number.values())) == i)[0] for i in range(20)]\n",
    "all_fine = np.concatenate(fine_by_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mc=10\n",
    "master_seed = 42\n",
    "np.random.seed(master_seed)\n",
    "seeds = np.random.randint(10000, size=n_mc)\n",
    "n_cores=30\n",
    "\n",
    "truth=np.array(list(fine_number_to_coarse_number.values()))\n",
    "\n",
    "tasksim_clusters = pickle.load(open('cifar_tasksim_clusters.pkl', 'rb'))\n",
    "condmean_clusters = pickle.load(open('cifar_condmean_clusters.pkl', 'rb'))\n",
    "\n",
    "n_trees_coarse = 100\n",
    "n_trees_fine = 25\n",
    "\n",
    "max_depths=[10,20,30]\n",
    "\n",
    "accuracy_tuples = []\n",
    "for i, pair_of_clusters in enumerate(zip(tasksim_clusters, condmean_clusters)):\n",
    "    n_classes = len(pair_of_clusters[0])\n",
    "    \n",
    "    n_tasksim_coarse = len(np.unique(pair_of_clusters[0]))\n",
    "    n_condmean_coarse = len(np.unique(pair_of_clusters[0]))\n",
    "    \n",
    "    max_coarse = max([n_tasksim_coarse, n_condmean_coarse])\n",
    "\n",
    "    for j, md in enumerate(max_depths):\n",
    "        #- Hierarchical forest trained using tasksim clusters\n",
    "        accuracy_tuples.append((seeds[i], pair_of_clusters[0], n_trees_coarse, n_trees_fine, md))\n",
    "\n",
    "        #- Hierarchical forest trained using random clusters with the same structure as the tasksim clusters\n",
    "        accuracy_tuples.append((seeds[i], \n",
    "                                pair_of_clusters[0][np.random.choice(n_classes, size=n_classes, replace=False)],\n",
    "                              n_trees_coarse, n_trees_fine, md))\n",
    "\n",
    "\n",
    "        #- Hierarchical forest trained using cond mean clusters\n",
    "        accuracy_tuples.append((seeds[i], pair_of_clusters[1], n_trees_coarse, n_trees_fine, md))\n",
    "\n",
    "        #- Hierarchical forest trained using random clusters with the same structure as the cond mean clusters\n",
    "        accuracy_tuples.append((seeds[i], \n",
    "                                pair_of_clusters[1][np.random.choice(n_classes, size=n_classes, replace=False)],\n",
    "                               n_trees_coarse, n_trees_fine, md))\n",
    "\n",
    "        #- Flat forest\n",
    "        accuracy_tuples.append((seeds[i], None, 0, n_trees_coarse + max_coarse * n_trees_fine, 2*md))\n",
    "    \n",
    "condensed_func = lambda x: get_accuracy(*x)\n",
    "    \n",
    "accuracies = Parallel(n_jobs=n_cores)(delayed(condensed_func)(tupl) for tupl in accuracy_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasksim_accuracies = np.zeros((len(max_depths), n_mc))\n",
    "random_tasksim_accuracies = np.zeros((len(max_depths), n_mc))\n",
    "\n",
    "condmean_accuracies = np.zeros((len(max_depths), n_mc))\n",
    "random_condmean_accuracies = np.zeros((len(max_depths), n_mc))\n",
    "\n",
    "flat_accuracies = np.zeros((len(max_depths), n_mc))\n",
    "\n",
    "for i in range(int(len(accuracies) / 5)):\n",
    "    index = i*5\n",
    "    max_depth_idx = i % 3\n",
    "    mc_idx = int(np.math.floor(i / 3))\n",
    "\n",
    "    tasksim_accuracies[max_depth_idx, mc_idx] = accuracies[index+0]\n",
    "    random_tasksim_accuracies[max_depth_idx, mc_idx] = accuracies[index+1]\n",
    "    condmean_accuracies[max_depth_idx, mc_idx] = accuracies[index+2]\n",
    "    random_condmean_accuracies[max_depth_idx, mc_idx] = accuracies[index+3]\n",
    "    flat_accuracies[max_depth_idx, mc_idx] = accuracies[index+4]\n",
    "\n",
    "print(\"(mean, std err) tasksim accuracy\", np.mean(tasksim_accuracies), np.std(tasksim_accuracies) / np.sqrt(n_mc))\n",
    "print(\"(mean, std err) random tasksim accuracy\", np.mean(random_tasksim_accuracies), np.std(random_tasksim_accuracies) / np.sqrt(n_mc))\n",
    "print(\"(mean, std err) condmean accuracy\", np.mean(condmean_accuracies), np.std(condmean_accuracies) / np.sqrt(n_mc))\n",
    "print(\"(mean, std err) random condmean accuracy\", np.mean(random_condmean_accuracies), np.std(random_condmean_accuracies) / np.sqrt(n_mc))\n",
    "print(\"(mean, std err) flat accuracy\", np.mean(flat_accuracies), np.std(flat_accuracies) / np.sqrt(n_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc",
   "language": "python",
   "name": "hc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
