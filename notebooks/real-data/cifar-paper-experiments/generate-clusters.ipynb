{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tasksim import *\n",
    "\n",
    "from graspologic.embed import AdjacencySpectralEmbed as ASE\n",
    "from graspologic.cluster import GaussianCluster as GMM\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def evaluate_clusters(f, truth, preds, calculate_random=False, n_mc=500, acorn=None):\n",
    "    eval_pred = f(truth, preds)\n",
    "    \n",
    "    if not calculate_random:\n",
    "        return eval_pred\n",
    "    \n",
    "    eval_random = np.zeros(n_mc)\n",
    "    for i in range(n_mc):\n",
    "        shuffled_preds = np.random.choice(preds, size=len(preds), replace=False)\n",
    "        eval_random[i] = f(truth, shuffled_preds)\n",
    "        \n",
    "    return eval_pred, np.mean(eval_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Data processing 1\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, file='cifar_100_Bit_m-r101x1_embd.p', train=True, classes=[]):\n",
    "        if train:\n",
    "            if file == '../../../data/cifar_100_Bit_m-r101x1_embd.p':\n",
    "                self.data = pickle.load(open(file, 'rb'))[0][0]\n",
    "                self.targets = np.concatenate(pickle.load(open(file, 'rb'))[0][1])\n",
    "        else:\n",
    "            if file == '../../../data/cifar_100_Bit_m-r101x1_embd.p':\n",
    "                self.data = pickle.load(open(file, 'rb'))[1][0]\n",
    "                self.targets = np.concatenate(pickle.load(open(file, 'rb'))[1][1])\n",
    "        \n",
    "        self.classes = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3d495b746143a6909ad12fab20c11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "#- Data processing 2\n",
    "\n",
    "cif100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
    "\n",
    "file='../../../data/cifar_100_Bit_m-r101x1_embd.p'\n",
    "trainset = Dataset(file, train=True, classes=cif100.classes)\n",
    "testset = Dataset(file, train=False, classes=cif100.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Data processing 3\n",
    "\n",
    "data_dimension=2048\n",
    "\n",
    "if data_dimension < trainset.data.shape[1]:\n",
    "    pca = PCA(n_components=data_dimension)\n",
    "    pca.fit(trainset.data)\n",
    "    trainset.data = pca.transform(trainset.data)\n",
    "    testset.data = pca.transform(testset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Data processing 4\n",
    "\n",
    "coarse_to_fine_map = {\n",
    "'aquatic_mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "'food_containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "'fruit_and_vegetables': ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
    "'household_electrical_devices': ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
    "'household_furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "'large_carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "'large_man-made_outdoor_things': ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
    "'large_natural_outdoor_scenes': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
    "'large_omnivores_and_herbivores': ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
    "'medium-sized_mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "'non-insect_invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
    "'vehicles_1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "'vehicles_2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
    "}\n",
    "\n",
    "coarse_number_to_coarse_name = {i: name for i, name in enumerate(coarse_to_fine_map)}\n",
    "\n",
    "def fine_to_coarse(coarse_to_fine):\n",
    "    fine_to_coarse_map = {}\n",
    "    for key in coarse_to_fine:\n",
    "        fines = coarse_to_fine[key]\n",
    "        for f in fines:\n",
    "            fine_to_coarse_map[f] = key\n",
    "            \n",
    "    return fine_to_coarse_map\n",
    "\n",
    "fine_to_coarse_map = fine_to_coarse(coarse_to_fine_map)\n",
    "\n",
    "fine_number_to_fine_name = {i: name for i, name in enumerate(trainset.classes)}\n",
    "fine_name_to_fine_number = {name: i for i, name in fine_number_to_fine_name.items()}\n",
    "\n",
    "for i in range(100):\n",
    "    fine_to_coarse_map[fine_number_to_fine_name[i]]\n",
    "    \n",
    "coarse_name_to_coarse_number = {name: i for i, name in enumerate(coarse_to_fine_map)}\n",
    "\n",
    "coarse_targets = np.array([coarse_name_to_coarse_number[fine_to_coarse_map[fine_number_to_fine_name[y]]] for y in trainset.targets])\n",
    "idx_by_coarse = np.array([np.where(coarse_targets == y)[0] for y in range(20)])\n",
    "idx_by_fine = np.array([np.where(trainset.targets == y)[0] for y in range(100)])\n",
    "\n",
    "\n",
    "test_coarse_targets = np.array([coarse_name_to_coarse_number[fine_to_coarse_map[fine_number_to_fine_name[y]]] for y in testset.targets])\n",
    "test_idx_by_coarse = np.array([np.where(test_coarse_targets == y)[0] for y in range(20)])\n",
    "\n",
    "\n",
    "coarse_names = np.array(list(coarse_name_to_coarse_number.keys()))\n",
    "\n",
    "fine_number_to_coarse_number = {fn: coarse_name_to_coarse_number[\n",
    "                                        fine_to_coarse_map[\n",
    "                                            fine_number_to_fine_name[fn]\n",
    "                                        ]\n",
    "                                    ] for fn in range(100)}\n",
    "\n",
    "\n",
    "fine_by_coarse = [np.where(np.array(list(fine_number_to_coarse_number.values())) == i)[0] for i in range(20)]\n",
    "all_fine = np.concatenate(fine_by_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "n_props = 0.1\n",
    "\n",
    "generate_dist_matrix_kwargs = {'metric':'tasksim', \n",
    "                               'metric_kwargs':{'n_neg_classes': 20, \n",
    "                                                'task_similarity_kwargs': {'transformer_kwargsx': \n",
    "                                                                               {'max_depth':4},\n",
    "                                                                          'transformer_kwargsz':\n",
    "                                                                              {'max_depth':4}}}, \n",
    "                               'function_tuples':None, \n",
    "                               'n_cores':30, \n",
    "                               'acorn':None\n",
    "                              }\n",
    "\n",
    "\n",
    "process_dist_matrix_kwargs = {'make_symmetric': True,\n",
    "                              'scale':True,\n",
    "                             'aug_diag':True,\n",
    "                             }\n",
    "\n",
    "embedding=ASE\n",
    "embedding_kwargs={'n_components':16}\n",
    "cluster=GMM\n",
    "cluster_kwargs = {'max_components': 30}\n",
    "\n",
    "cluster_dists_kwargs = {'embedding':embedding, \n",
    "                        'embedding_kwargs':embedding_kwargs, \n",
    "                        'cluster':cluster, \n",
    "                        'cluster_kwargs':cluster_kwargs\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.88it/s]\n"
     ]
    }
   ],
   "source": [
    "n_mc=10\n",
    "master_seed = 42\n",
    "np.random.seed(master_seed)\n",
    "seeds = np.random.randint(10000, size=n_mc)\n",
    "\n",
    "generate_tasksim=False\n",
    "generate_condmean=False\n",
    "\n",
    "if generate_tasksim:\n",
    "    tasksim_clusters = []\n",
    "\n",
    "if generate_condmean:\n",
    "    condmean_clusters = []\n",
    "\n",
    "    \n",
    "#- Generate clusters\n",
    "for iteration in tqdm(range(n_mc)):\n",
    "    start = time.time()\n",
    "    seed =  seeds[iteration]\n",
    "    X_train, _, y_train, _ = train_test_split(trainset.data, trainset.targets, test_size=0.9, random_state=seed)\n",
    "    \n",
    "    if generate_tasksim:\n",
    "        temp_tasksim = generate_hierarchy(X_train, y_train,\n",
    "                                     generate_dist_matrix_kwargs, process_dist_matrix_kwargs, cluster_dists_kwargs)\n",
    "        tasksim_clusters.append(temp_tasksim)\n",
    "        \n",
    "        pickle.dump(tasksim_clusters, open('cifar_tasksim_clusters.pkl', 'wb'))\n",
    "    \n",
    "    \n",
    "    if generate_condmean:\n",
    "        pca = PCA(n_components=data_dimension)\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        unique_y = np.unique(y_train)\n",
    "        \n",
    "        conditional_means = np.array([np.mean(X_train[np.where(y_train == c)[0]], axis=0) for c in unique_y])\n",
    "        gmm = GMM(min_components=10, max_components=30, reg_covar=1e-4)\n",
    "        temp_condmean = gmm.fit_predict(conditional_means)\n",
    "        condmean_clusters.append(temp_condmean)\n",
    "        \n",
    "        pickle.dump(condmean_clusters, open('cifar_condmean_clusters.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasksim\n",
      "means [3.28826143e-01 5.51670376e-06]\n",
      "std errs [0.01501901 0.00014459]\n",
      "\n",
      "condmeans\n",
      "means [2.95185455e-01 3.41157506e-05]\n",
      "std errs [0.00767691 0.00011096]\n"
     ]
    }
   ],
   "source": [
    "f=ARI\n",
    "truth=np.array(list(fine_number_to_coarse_number.values()))\n",
    "\n",
    "tasksim_clusters = pickle.load(open('cifar_tasksim_clusters.pkl', 'rb'))\n",
    "tasksim_evals = np.zeros((2, n_mc))\n",
    "for i, clust_ in enumerate(tasksim_clusters):\n",
    "    tasksim_evals[:, i] = evaluate_clusters(f, truth, clust_, calculate_random=True, n_mc=1000)\n",
    "    \n",
    "condmean_clusters = pickle.load(open('cifar_condmean_clusters.pkl', 'rb'))\n",
    "condmean_evals = np.zeros((2, n_mc))\n",
    "for i, clust_ in enumerate(condmean_clusters):\n",
    "    condmean_evals[:, i] = evaluate_clusters(f, truth, clust_, calculate_random=True, n_mc=1000)\n",
    "    \n",
    "print(\"tasksim\")\n",
    "print(\"means\", np.mean(tasksim_evals, axis=-1))\n",
    "print(\"std errs\", np.std(tasksim_evals, axis=-1) / np.sqrt(n_mc))\n",
    "print()\n",
    "print(\"condmeans\")\n",
    "print(\"means\", np.mean(condmean_evals, axis=-1))\n",
    "print(\"std errs\", np.std(condmean_evals, axis=-1) / np.sqrt(n_mc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc",
   "language": "python",
   "name": "hc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
